{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincekillerz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gharib-ml/Desktop/master_thesis_v2/wandb/run-20240702_112512-h16lubxt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vincekillerz/master_thesis_v2/runs/h16lubxt' target=\"_blank\">comfy-forest-110</a></strong> to <a href='https://wandb.ai/vincekillerz/master_thesis_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vincekillerz/master_thesis_v2' target=\"_blank\">https://wandb.ai/vincekillerz/master_thesis_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vincekillerz/master_thesis_v2/runs/h16lubxt' target=\"_blank\">https://wandb.ai/vincekillerz/master_thesis_v2/runs/h16lubxt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact saved_model:v11, 126.27MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.4\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "from src.tools import check_file_path\n",
    "\n",
    "url=\"vincekillerz/traversability-estimation-v4/saved_model:v11\"\n",
    "name=\"asymformer\"+\"-\"+url.split(\":\")[-1]\n",
    "run = wandb.init() \n",
    "            \n",
    "artifact = run.use_artifact(url, type='model')\n",
    "artifact_dir = artifact.download()\n",
    "config = yaml.safe_load(open(check_file_path(artifact_dir, 'config.yaml')))\n",
    "model_path = check_file_path(artifact_dir, 'trained_model.pth')\n",
    "# Some standard imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gharib-ml/Desktop/master_thesis_v2/src/models/asymformer/convnext.py:159: UserWarning: Overwriting convnext_tiny in registry with src.models.asymformer.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n",
      "/home/gharib-ml/Desktop/master_thesis_v2/src/models/asymformer/convnext.py:168: UserWarning: Overwriting convnext_small in registry with src.models.asymformer.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_small(pretrained=False,in_22k=False, **kwargs):\n",
      "/home/gharib-ml/Desktop/master_thesis_v2/src/models/asymformer/convnext.py:177: UserWarning: Overwriting convnext_base in registry with src.models.asymformer.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
      "/home/gharib-ml/Desktop/master_thesis_v2/src/models/asymformer/convnext.py:186: UserWarning: Overwriting convnext_large in registry with src.models.asymformer.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
      "/home/gharib-ml/Desktop/master_thesis_v2/src/models/asymformer/convnext.py:195: UserWarning: Overwriting convnext_xlarge in registry with src.models.asymformer.convnext.convnext_xlarge. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gharib-ml/Desktop/master_thesis_v2/artifacts/saved_model:v11/trained_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B0_T(\n",
       "  (down_sample_1): down_sample_block(\n",
       "    (depth_stem): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(1, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (rgb_stem): Sequential(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm()\n",
       "    )\n",
       "    (rgb_layer): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.018)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.035)\n",
       "      )\n",
       "    )\n",
       "    (depth_layer): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (kv): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.014)\n",
       "        (norm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (depth_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (down_sample_2): down_sample_block(\n",
       "    (depth_stem): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (rgb_stem): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (rgb_layer): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.053)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.071)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.088)\n",
       "      )\n",
       "    )\n",
       "    (depth_layer): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.029)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.043)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (depth_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "    (SCC): SCC_Module(\n",
       "      (fus_atten): SpatialAttention_max(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc_spatial): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=False)\n",
       "        )\n",
       "        (fc_channel): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=32, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cross_atten): Cross_Atten_Lite_split(\n",
       "        (bn_x1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_x2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (kq1): Linear(in_features=64, out_features=96, bias=True)\n",
       "        (kq2): Linear(in_features=192, out_features=96, bias=True)\n",
       "        (v_conv): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (out_conv): Linear(in_features=32, out_features=64, bias=True)\n",
       "        (bn_last): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_sample_3): down_sample_block(\n",
       "    (depth_stem): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (rgb_stem): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (rgb_layer): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.106)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.124)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.141)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.159)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.176)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.194)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.212)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.229)\n",
       "      )\n",
       "      (8): Block(\n",
       "        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.247)\n",
       "      )\n",
       "    )\n",
       "    (depth_layer): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (kv): Linear(in_features=160, out_features=320, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.057)\n",
       "        (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (kv): Linear(in_features=160, out_features=320, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.071)\n",
       "        (norm2): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (depth_norm): LayerNorm((160,), eps=1e-06, elementwise_affine=True)\n",
       "    (SCC): SCC_Module(\n",
       "      (fus_atten): SpatialAttention_max(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc_spatial): Sequential(\n",
       "          (0): Linear(in_features=544, out_features=34, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=34, out_features=544, bias=False)\n",
       "        )\n",
       "        (fc_channel): Sequential(\n",
       "          (0): Linear(in_features=544, out_features=68, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=68, out_features=544, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(544, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cross_atten): Cross_Atten_Lite_split(\n",
       "        (bn_x1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_x2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (kq1): Linear(in_features=160, out_features=192, bias=True)\n",
       "        (kq2): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (v_conv): Linear(in_features=160, out_features=80, bias=True)\n",
       "        (out_conv): Linear(in_features=80, out_features=160, bias=True)\n",
       "        (bn_last): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down_sample_4): down_sample_block(\n",
       "    (depth_stem): OverlapPatchEmbed(\n",
       "      (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (rgb_stem): Sequential(\n",
       "      (0): LayerNorm()\n",
       "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (rgb_layer): Sequential(\n",
       "      (0): Block(\n",
       "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.265)\n",
       "      )\n",
       "      (1): Block(\n",
       "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.282)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "        (norm): LayerNorm()\n",
       "        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop_path): DropPath(drop_prob=0.300)\n",
       "      )\n",
       "    )\n",
       "    (depth_layer): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.086)\n",
       "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.100)\n",
       "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dwconv): DWConv(\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (depth_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "    (SCC): SCC_Module(\n",
       "      (fus_atten): SpatialAttention_max(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc_spatial): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=64, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=64, out_features=1024, bias=False)\n",
       "        )\n",
       "        (fc_channel): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=128, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cross_atten): Cross_Atten_Lite_split(\n",
       "        (bn_x1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn_x2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (kq1): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (kq2): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (v_conv): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_conv): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (bn_last): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder): DecoderHead(\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (linear_c4): MLP(\n",
       "      (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear_c3): MLP(\n",
       "      (proj): Linear(in_features=160, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear_c2): MLP(\n",
       "      (proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear_c1): MLP(\n",
       "      (proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear_fuse): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (linear_pred): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.model_builder import model_builder\n",
    "\n",
    "\n",
    "config['ml_orchestrator']['device']= \"cuda:1\"\n",
    "device = torch.device(config['ml_orchestrator']['device'])\n",
    "model = model_builder(config)\n",
    "model.to(device)\n",
    "\n",
    "# load model\n",
    "print(model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def benchmark(model, input_shape=(1024, 3, 512, 512), dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            pred_loc  = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%10==0:\n",
    "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    print('Average throughput: %.2f images/second'%(input_shape[0]/np.mean(timings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1, 3, 480, 640).to(device)\n",
    "depth = torch.randn(1, 1, 480, 640).to(device)\n",
    "masks = torch.randn(1, 1, 480, 640).to(device)\n",
    "segs = torch.randn(1, 1, 480, 640).to(device)\n",
    "torch_out = model(image,depth)\n",
    "model_path_output =\"./models/\"+name+'.onnx'\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  (image,depth),                         # model input (or a tuple for multiple inputs)\n",
    "                  model_path_output,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=15,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input_rgb','input_depth'],   # the model's input names\n",
    "                  output_names = ['output'],# the model's output names\n",
    "                #   dynamic_axes={'input_rgb' : {0 : 'batch_size'},    # variable length axes\n",
    "                #                 'input_depth' : {0 : 'batch_size'},\n",
    "                #                 'output' : {0 : 'batch_size'}})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(model_path_output)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(model_path_output, providers=[(\"CUDAExecutionProvider\")])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {\n",
    "    ort_session.get_inputs()[0].name: to_numpy(image),\n",
    "    ort_session.get_inputs()[1].name: to_numpy(depth)\n",
    "    \n",
    "}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-02, atol=1e-02)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in execution: Got invalid dimensions for input: input_rgb for the following indices\n index: 0 Got: 4 Expected: 1\n Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     59\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 60\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_iobinding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio_binding\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace '_' with your ONNX model's actual output variable if you want to use it\u001b[39;00m\n\u001b[1;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     62\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Desktop/master_thesis_v2/venv2/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:331\u001b[0m, in \u001b[0;36mSession.run_with_iobinding\u001b[0;34m(self, iobinding, run_options)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_iobinding\u001b[39m(\u001b[38;5;28mself\u001b[39m, iobinding, run_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    Compute the predictions.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    :param iobinding: the iobinding object that has graph inputs/outputs bind.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    :param run_options: See :class:`onnxruntime.RunOptions`.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_iobinding\u001b[49m\u001b[43m(\u001b[49m\u001b[43miobinding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iobinding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in execution: Got invalid dimensions for input: input_rgb for the following indices\n index: 0 Got: 4 Expected: 1\n Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# Lists to store execution times\n",
    "pytorch_times = []\n",
    "onnx_times = []\n",
    "print(device)\n",
    "\n",
    "batch_size = 4\n",
    "x = torch.randn(batch_size, 3, 480, 640).to(device)\n",
    "y = torch.randn(batch_size, 1, 480, 640).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "start_time = time.time()\n",
    "for i in range(100):  # Conducting 10 tests\n",
    "    # Generate random inputs\n",
    "\n",
    "    # Measure PyTorch inference time\n",
    "    # torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x, y)  # Replace '_' with your model's actual output variable if you want to use it\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    pytorch_times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "\n",
    "    # # Prepare inputs for ONNX Runtime\n",
    "    # ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x), \n",
    "    #               ort_session.get_inputs()[1].name: to_numpy(y)}\n",
    "\n",
    "    io_binding = ort_session.io_binding()\n",
    "    io_binding.bind_input(\n",
    "        name=\"input_rgb\",\n",
    "        device_type=\"cuda\",\n",
    "        device_id=0,\n",
    "        element_type=np.float32,\n",
    "        shape=tuple(x.shape),\n",
    "        buffer_ptr=x.data_ptr(),\n",
    "    )\n",
    "    io_binding.bind_input(\n",
    "        name=\"input_depth\",\n",
    "        device_type=\"cuda\",\n",
    "        device_id=0,\n",
    "        element_type=np.float32,\n",
    "        shape=tuple(y.shape),\n",
    "        buffer_ptr=y.data_ptr(),\n",
    "    )\n",
    "    logit_output_shape = (4,1,480, 640)\n",
    "    logit_output = torch.empty(logit_output_shape, dtype=torch.float32, device='cuda').contiguous()\n",
    "    io_binding.bind_output(\n",
    "        name=\"output\",\n",
    "        device_type=\"cuda\",\n",
    "        device_id=0,\n",
    "        element_type=np.float32,\n",
    "        shape=tuple(logit_output.shape),\n",
    "        buffer_ptr=logit_output.data_ptr()\n",
    "    )\n",
    "    # Measure ONNX Runtime inference time\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    _ = ort_session.run_with_iobinding(io_binding)  # Replace '_' with your ONNX model's actual output variable if you want to use it\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    onnx_times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "\n",
    "pytorch_times = pytorch_times[5:]\n",
    "onnx_times = onnx_times[5:]\n",
    "# Plot comparison of execution times\n",
    "plt.plot(pytorch_times, label='PyTorch', marker='o')\n",
    "plt.plot(onnx_times, label='ONNX Runtime', marker='x')\n",
    "plt.legend()\n",
    "plt.xlabel('Test Case')\n",
    "plt.ylabel('Inference Time (ms)')\n",
    "plt.title('PyTorch vs ONNX Runtime Inference Time Comparison')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# compare mean\n",
    "pytorch_mean = np.mean(pytorch_times)\n",
    "onnx_mean = np.mean(onnx_times)\n",
    "print(f\"ONNX mean inference time: {onnx_mean:.2f} ms\")\n",
    "print(f\"PyTorch mean inference time: {pytorch_mean:.2f} ms\")\n",
    "print(f\"ONNX is {pytorch_mean/onnx_mean:.2f}x faster than PyTorch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node /down_sample_1/rgb_layer/rgb_layer.2/dwconv/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_1/depth_layer.0/attn/kv/Add (Add): Weight truncated in fp16\n",
      "Node /down_sample_2/rgb_stem/rgb_stem.1/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_2/rgb_layer/rgb_layer.0/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_2/rgb_layer/rgb_layer.0/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_2/rgb_layer/rgb_layer.2/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_2/depth_layer.1/attn/kv/Add (Add): Weight truncated in fp16\n",
      "Node /down_sample_2/SCC/cross_atten/bn_x1/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_2/SCC/cross_atten/bn_x2/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_2/SCC/cross_atten/bn_x2/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_stem/rgb_stem.1/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_stem/rgb_stem.1/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.0/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.0/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.0/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.1/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.1/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.2/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.3/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.4/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.4/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.4/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.5/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.5/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.6/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.6/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.6/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.6/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.6/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.7/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.7/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.7/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.7/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.7/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.8/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.8/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.8/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_3/rgb_layer/rgb_layer.8/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_3/depth_stem/proj/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_3/depth_layer.0/attn/kv/Add (Add): Weight truncated in fp16\n",
      "Node /down_sample_3/depth_layer.0/mlp/fc2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/depth_layer.1/attn/sr/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_3/depth_layer.1/attn/kv/Add (Add): Weight truncated in fp16\n",
      "Node /down_sample_3/depth_layer.1/mlp/fc1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/depth_layer.1/mlp/fc2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_3/SCC/conv1/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_3/SCC/cross_atten/bn_x2/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_3/SCC/cross_atten/bn_x2/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_stem/rgb_stem.1/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_stem/rgb_stem.1/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.0/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.0/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.0/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.0/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.1/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.1/norm/LayerNormalization (LayerNormalization): Weight overflow in fp16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3108576/3818076605.py:6: RuntimeWarning: overflow encountered in cast\n",
      "  fp16_data = original_data.astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node /down_sample_4/rgb_layer/rgb_layer.1/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.1/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.1/pwconv2/Add (Add): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.1/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.2/dwconv/Conv (Conv): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.2/norm/LayerNormalization (LayerNormalization): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.2/norm/LayerNormalization (LayerNormalization): Weight overflow in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.2/pwconv1/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.2/pwconv2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/rgb_layer/rgb_layer.2/Mul (Mul): Weight overflow in fp16\n",
      "Node /down_sample_4/depth_layer.0/attn/kv/Add (Add): Weight truncated in fp16\n",
      "Node /down_sample_4/depth_layer.1/attn/kv/Add (Add): Weight truncated in fp16\n",
      "Node /down_sample_4/depth_layer.1/attn/proj/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/depth_layer.1/mlp/fc2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/SCC/fus_atten/fc_spatial/fc_spatial.0/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/SCC/fus_atten/fc_spatial/fc_spatial.2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/SCC/conv1/Conv (Conv): Weight truncated in fp16\n",
      "Node /down_sample_4/SCC/cross_atten/bn_x1/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_4/SCC/cross_atten/bn_x2/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_4/SCC/cross_atten/bn_x2/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /down_sample_4/SCC/cross_atten/kq2/MatMul (MatMul): Weight truncated in fp16\n",
      "Node /down_sample_4/SCC/cross_atten/bn_last/BatchNormalization (BatchNormalization): Weight overflow in fp16\n",
      "Node /Decoder/linear_fuse/linear_fuse.0/Conv (Conv): Weight truncated in fp16\n",
      "个数： 79\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "\n",
    "def is_overflow_in_fp16(tensor):\n",
    "    original_data = np.asarray(tensor, dtype=np.float32)\n",
    "    fp16_data = original_data.astype(np.float16)\n",
    "    back_converted_data = fp16_data.astype(np.float32)\n",
    "\n",
    "    diff = np.abs(back_converted_data - original_data)\n",
    "    return np.any(diff > 0.001)\n",
    "\n",
    "def is_truncated_in_fp16(tensor):\n",
    "    original_data = np.asarray(tensor, dtype=np.float32)\n",
    "\n",
    "    return np.any(np.abs(original_data) <= 0.0000001)  # Check if the FP16 weight is zero\n",
    "\n",
    "\n",
    "\n",
    "model = onnx.load(model_path_output)  # Load ONNX model\n",
    "overflow_list=[]\n",
    "for node in model.graph.node:\n",
    "    if node.input:  # Check network layer which has 'input'\n",
    "        for input_name in node.input:\n",
    "            weight = next((init for init in model.graph.initializer if init.name == input_name), None)\n",
    "            if weight is not None:  # Make sure the layer has 'weight'\n",
    "                weights = onnx.numpy_helper.to_array(weight)\n",
    "                if is_overflow_in_fp16(weights):\n",
    "                    print(f\"Node {node.name} ({node.op_type}): Weight overflow in fp16\")\n",
    "                    overflow_list.append(node.name)\n",
    "                if is_truncated_in_fp16(weights):\n",
    "                    print(f\"Node {node.name} ({node.op_type}): Weight truncated in fp16\")\n",
    "                    overflow_list.append(node.name)\n",
    "\n",
    "print('个数：',len(overflow_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorrt as trt\n",
    "\n",
    "def build_engine(onnx_file_path, engine_file_path, overflow_list, flop=16):\n",
    "    trt_logger = trt.Logger(trt.Logger.WARNING)  # trt.Logger.ERROR\n",
    "    builder = trt.Builder(trt_logger)\n",
    "    builder_config = builder.create_builder_config()\n",
    "    \n",
    "    network = builder.create_network(\n",
    "        1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    )\n",
    "    \n",
    "    parser = trt.OnnxParser(network, trt_logger)\n",
    "    # parse ONNX\n",
    "    with open(onnx_file_path, 'rb') as model:\n",
    "        if not parser.parse(model.read()):\n",
    "            print('ERROR: Failed to parse the ONNX file.')\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            return None\n",
    "    print(\"Completed parsing ONNX file\")\n",
    "\n",
    "    # default = 1 for fixed batch size\n",
    "    builder.max_batch_size = 1\n",
    "    # set mixed flop computation for the best performance\n",
    "\n",
    "    builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "    if os.path.isfile(engine_file_path):\n",
    "        try:\n",
    "            os.remove(engine_file_path)\n",
    "        except Exception:\n",
    "            print(\"Cannot remove existing file: \",\n",
    "                engine_file_path)\n",
    "\n",
    "    print(\"Creating Tensorrt Engine\")\n",
    "\n",
    "    for layer in network:\n",
    "        for layer_name in overflow_list:\n",
    "            if layer_name in layer.name:\n",
    "                layer.precision = trt.float32\n",
    "                print(f'Network Layer: {layer.name}, {layer.type}, {layer.precision}, is_set: {layer.precision_is_set}')\n",
    "\n",
    "    config = builder.create_builder_config()\n",
    "    config.set_tactic_sources(1 << int(trt.TacticSource.CUBLAS))\n",
    "    config.max_workspace_size = 2 << 30\n",
    "    config.set_flag(trt.BuilderFlag.FP16)\n",
    "    config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n",
    "\n",
    "    print('config.flags: ', config.flags)\n",
    "    profile = builder.create_optimization_profile()\n",
    "\n",
    "\n",
    "\n",
    "    profile.set_shape(\"input_rgb\", (1, 3, 480, 640), (1, 3, 480, 640),(1, 3, 480, 640))   \n",
    "    profile.set_shape(\"input_depth\", (1, 1, 480, 640), (1, 1, 480, 640),(1, 1, 480, 640))   \n",
    "    profile.set_shape(\"output\", (1, 1, 480, 640), (1, 1, 480, 640),(30, 1, 480, 640)) \n",
    "\n",
    "\n",
    "    config.add_optimization_profile(profile)\n",
    "    engine = builder.build_engine(network, config)\n",
    "    with open(engine_file_path, \"wb\") as f:\n",
    "        f.write(engine.serialize())\n",
    "    print(\"Serialized Engine Saved at: \", engine_file_path)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/20/2024-18:38:15] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "Completed parsing ONNX file\n",
      "Creating Tensorrt Engine\n",
      "Network Layer: /down_sample_1/rgb_layer/rgb_layer.2/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_1/depth_layer.0/attn/kv/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/rgb_stem/rgb_stem.1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/rgb_layer/rgb_layer.0/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/rgb_layer/rgb_layer.0/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/rgb_layer/rgb_layer.2/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/depth_layer.1/attn/kv/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/SCC/cross_atten/bn_x1/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/SCC/cross_atten/bn_x2/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_2/SCC/cross_atten/bn_x2/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_stem/rgb_stem.1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_stem/rgb_stem.1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.0/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.0/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.0/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.1/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.1/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.2/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.3/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.4/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.4/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.4/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.5/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.5/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.6/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.6/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.6/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.6/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.6/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.7/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.7/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.7/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.7/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.7/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.8/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.8/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.8/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/rgb_layer/rgb_layer.8/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_stem/proj/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_layer.0/attn/kv/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_layer.0/mlp/fc2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_layer.1/attn/sr/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_layer.1/attn/kv/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_layer.1/mlp/fc1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/depth_layer.1/mlp/fc2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/SCC/conv1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/SCC/cross_atten/bn_x2/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_3/SCC/cross_atten/bn_x2/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_stem/rgb_stem.1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_stem/rgb_stem.1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.0/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.0/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.0/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.0/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.1/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.1/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.1/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.1/pwconv2/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.1/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.2/dwconv/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.2/pwconv1/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.2/pwconv2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/rgb_layer/rgb_layer.2/Mul, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/depth_layer.0/attn/kv/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/depth_layer.1/attn/kv/Add, LayerType.ELEMENTWISE, DataType.FLOAT, is_set: True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3108576/1555057692.py:24: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  builder.max_batch_size = 1\n",
      "/tmp/ipykernel_3108576/1555057692.py:46: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 2 << 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network Layer: /down_sample_4/depth_layer.1/attn/proj/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/depth_layer.1/mlp/fc2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/fus_atten/fc_spatial/fc_spatial.0/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/fus_atten/fc_spatial/fc_spatial.2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/conv1/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/cross_atten/bn_x1/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/cross_atten/bn_x2/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/cross_atten/bn_x2/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/cross_atten/kq2/MatMul, LayerType.MATRIX_MULTIPLY, DataType.FLOAT, is_set: True\n",
      "Network Layer: /down_sample_4/SCC/cross_atten/bn_last/BatchNormalization, LayerType.SCALE, DataType.FLOAT, is_set: True\n",
      "Network Layer: /Decoder/linear_fuse/linear_fuse.0/Conv, LayerType.CONVOLUTION, DataType.FLOAT, is_set: True\n",
      "config.flags:  145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3108576/1555057692.py:61: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/20/2024-18:38:16] [TRT] [W] Detected layernorm nodes in FP16: /down_sample_2/rgb_layer/rgb_layer.0/norm/ReduceMean_1, /down_sample_4/depth_stem/norm/Sub, /down_sample_4/depth_stem/norm/Pow, /down_sample_4/depth_stem/norm/ReduceMean_1, /down_sample_4/depth_stem/norm/Add, /down_sample_1/depth_layer.0/attn/norm/Sub, /down_sample_1/depth_layer.0/attn/norm/Pow, /down_sample_1/depth_layer.0/attn/norm/Add, /down_sample_1/depth_layer.0/attn/norm/Sqrt, /down_sample_1/depth_layer.0/attn/norm/Div, /down_sample_1/rgb_layer/rgb_layer.1/norm/Sub, /down_sample_1/depth_layer.0/attn/norm/Mul, /down_sample_1/rgb_layer/rgb_layer.1/norm/Pow, /down_sample_1/depth_layer.0/attn/norm/Add_1, /down_sample_1/rgb_layer/rgb_layer.1/norm/Add, /down_sample_1/rgb_layer/rgb_layer.1/norm/Sqrt, /down_sample_1/rgb_layer/rgb_layer.1/norm/Div, /down_sample_1/rgb_layer/rgb_layer.1/norm/Mul, /down_sample_1/rgb_layer/rgb_layer.1/norm/Add_1, /down_sample_1/depth_layer.0/norm2/Sub, /down_sample_1/depth_layer.0/norm2/Pow, /down_sample_1/depth_layer.0/norm2/Add, /down_sample_1/rgb_layer/rgb_layer.2/norm/Sub, /down_sample_1/depth_layer.0/norm2/Sqrt, /down_sample_1/rgb_layer/rgb_layer.2/norm/Pow, /down_sample_1/depth_layer.0/norm2/Div, /down_sample_1/depth_layer.0/norm2/Mul, /down_sample_1/rgb_layer/rgb_layer.2/norm/Add, /down_sample_1/depth_layer.0/norm2/Add_1, /down_sample_1/rgb_layer/rgb_layer.2/norm/Sqrt, /down_sample_1/rgb_layer/rgb_layer.2/norm/Div, /down_sample_1/rgb_layer/rgb_layer.2/norm/Mul, /down_sample_1/rgb_layer/rgb_layer.2/norm/Add_1, /down_sample_2/rgb_layer/rgb_layer.1/norm/ReduceMean_1, /down_sample_1/rgb_stem/rgb_stem.1/ReduceMean_1, /down_sample_1/rgb_layer/rgb_layer.1/norm/ReduceMean_1, /down_sample_1/depth_stem/norm/ReduceMean_1, /down_sample_1/rgb_layer/rgb_layer.0/norm/ReduceMean_1, /down_sample_2/depth_layer.1/attn/norm/Sub, /down_sample_2/depth_layer.1/attn/norm/Pow, /down_sample_2/depth_layer.1/attn/norm/ReduceMean_1, /down_sample_2/depth_layer.1/attn/norm/Add, /down_sample_2/depth_layer.1/attn/norm/Sqrt, /down_sample_2/depth_layer.1/attn/norm/Div, /down_sample_2/depth_layer.1/attn/norm/Mul, /down_sample_2/depth_layer.1/attn/norm/Add_1, /down_sample_3/rgb_layer/rgb_layer.4/norm/Sub, /down_sample_3/rgb_layer/rgb_layer.4/norm/Pow, /down_sample_3/rgb_layer/rgb_layer.4/norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.4/norm/Add, /down_sample_3/rgb_layer/rgb_layer.4/norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.4/norm/Div, /down_sample_3/rgb_layer/rgb_layer.4/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.4/norm/Add_1, /down_sample_4/depth_layer.1/norm1/Sub, /down_sample_4/depth_layer.1/norm1/Pow, /down_sample_4/depth_layer.1/norm1/ReduceMean_1, /down_sample_4/depth_layer.1/norm1/Add, /down_sample_4/depth_layer.1/norm1/Sqrt, /down_sample_4/depth_layer.1/norm1/Div, /down_sample_4/depth_layer.1/norm1/Mul, /down_sample_4/depth_layer.1/norm1/Add_1, /down_sample_4/depth_layer.1/norm2/Sub, /down_sample_4/depth_layer.1/norm2/Pow, /down_sample_4/depth_layer.1/norm2/ReduceMean_1, /down_sample_4/depth_layer.1/norm2/Add, /down_sample_4/depth_layer.1/norm2/Sqrt, /down_sample_4/depth_layer.1/norm2/Div, /down_sample_4/depth_layer.1/norm2/Mul, /down_sample_4/depth_layer.1/norm2/Add_1, /down_sample_4/depth_norm/Sub, /down_sample_4/depth_norm/Pow, /down_sample_4/depth_norm/ReduceMean_1, /down_sample_4/depth_norm/Add, /down_sample_2/depth_layer.1/norm2/Sub, /down_sample_2/depth_layer.1/norm2/Pow, /down_sample_2/depth_layer.1/norm2/ReduceMean_1, /down_sample_2/depth_layer.1/norm2/Add, /down_sample_2/depth_layer.1/norm2/Sqrt, /down_sample_2/depth_layer.1/norm2/Div, /down_sample_2/depth_layer.1/norm2/Mul, /down_sample_3/rgb_layer/rgb_layer.5/norm/Sub, /down_sample_2/depth_layer.1/norm2/Add_1, /down_sample_3/rgb_layer/rgb_layer.5/norm/Pow, /down_sample_3/rgb_layer/rgb_layer.5/norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.5/norm/Add, /down_sample_3/rgb_layer/rgb_layer.5/norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.5/norm/Div, /down_sample_3/rgb_layer/rgb_layer.5/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.5/norm/Add_1, /down_sample_2/depth_norm/Sub, /down_sample_2/rgb_layer/rgb_layer.2/norm/ReduceMean_1, /down_sample_2/depth_layer.0/norm2/ReduceMean_1, /down_sample_2/rgb_layer/rgb_layer.0/norm/Mul, /down_sample_2/rgb_layer/rgb_layer.0/norm/Add_1, /down_sample_1/depth_layer.1/attn/norm/Add, /down_sample_1/depth_layer.1/attn/norm/Sqrt, /down_sample_1/depth_layer.1/attn/norm/Div, /down_sample_1/depth_layer.1/attn/norm/Mul, /down_sample_1/depth_layer.1/attn/norm/Add_1, /down_sample_2/rgb_layer/rgb_layer.1/norm/Sub, /down_sample_2/rgb_layer/rgb_layer.1/norm/Pow, /down_sample_2/rgb_layer/rgb_layer.1/norm/Add, /down_sample_2/rgb_layer/rgb_layer.1/norm/Sqrt, /down_sample_2/rgb_layer/rgb_layer.1/norm/Div, /down_sample_2/rgb_layer/rgb_layer.1/norm/Mul, /down_sample_1/depth_layer.1/norm2/Sub, /down_sample_2/rgb_layer/rgb_layer.1/norm/Add_1, /down_sample_1/depth_layer.1/norm2/Pow, /down_sample_1/depth_layer.1/norm2/Add, /down_sample_1/depth_layer.1/norm2/Sqrt, /down_sample_1/depth_layer.1/norm2/Div, /down_sample_1/depth_layer.1/norm2/Mul, /down_sample_1/depth_layer.1/norm2/Add_1, /down_sample_1/depth_layer.1/attn/norm/Sub, /down_sample_1/depth_norm/ReduceMean_1, /down_sample_1/rgb_layer/rgb_layer.2/norm/ReduceMean_1, /down_sample_2/rgb_layer/rgb_layer.0/norm/Div, /down_sample_2/rgb_stem/rgb_stem.0/Sub, /down_sample_2/rgb_stem/rgb_stem.0/Pow, /down_sample_1/depth_layer.1/norm1/Sub, /down_sample_2/rgb_stem/rgb_stem.0/Add, /down_sample_1/depth_layer.1/norm1/Pow, /down_sample_2/rgb_stem/rgb_stem.0/Sqrt, /down_sample_2/rgb_stem/rgb_stem.0/Div, /down_sample_1/depth_layer.1/norm1/Add, /down_sample_2/rgb_stem/rgb_stem.0/Mul, /down_sample_1/depth_layer.1/norm1/Sqrt, /down_sample_2/rgb_stem/rgb_stem.0/Add_1, /down_sample_1/depth_layer.1/norm1/Div, /down_sample_1/depth_layer.1/norm1/Mul, /down_sample_1/depth_layer.1/norm1/Add_1, /down_sample_2/rgb_layer/rgb_layer.0/norm/Sub, /down_sample_2/rgb_layer/rgb_layer.0/norm/Pow, /down_sample_2/rgb_layer/rgb_layer.0/norm/Add, /down_sample_2/rgb_layer/rgb_layer.0/norm/Sqrt, /down_sample_1/depth_layer.1/attn/norm/Pow, /down_sample_1/depth_layer.0/attn/norm/ReduceMean_1, /down_sample_1/depth_layer.0/norm2/ReduceMean_1, /down_sample_1/depth_layer.0/norm1/ReduceMean_1, /down_sample_2/rgb_stem/rgb_stem.0/ReduceMean_1, /down_sample_1/rgb_stem/rgb_stem.1/Sqrt, /down_sample_1/rgb_stem/rgb_stem.1/Sub, /down_sample_1/rgb_stem/rgb_stem.1/Pow, /down_sample_1/rgb_stem/rgb_stem.1/Add, /down_sample_1/rgb_stem/rgb_stem.1/Div, /down_sample_1/depth_stem/norm/Sub, /down_sample_1/rgb_stem/rgb_stem.1/Mul, /down_sample_1/depth_stem/norm/Pow, /down_sample_1/rgb_stem/rgb_stem.1/Add_1, /down_sample_1/depth_stem/norm/Add, /down_sample_1/depth_stem/norm/Div, /down_sample_1/rgb_layer/rgb_layer.0/norm/Sub, /down_sample_1/depth_stem/norm/Mul, /down_sample_1/rgb_layer/rgb_layer.0/norm/Pow, /down_sample_1/depth_stem/norm/Add_1, /down_sample_1/rgb_layer/rgb_layer.0/norm/Add, /down_sample_1/depth_layer.0/norm1/Sub, /down_sample_1/rgb_layer/rgb_layer.0/norm/Sqrt, /down_sample_1/depth_layer.0/norm1/Pow, /down_sample_1/rgb_layer/rgb_layer.0/norm/Div, /down_sample_1/rgb_layer/rgb_layer.0/norm/Mul, /down_sample_1/depth_layer.0/norm1/Add, /down_sample_1/rgb_layer/rgb_layer.0/norm/Add_1, /down_sample_1/depth_layer.0/norm1/Sqrt, /down_sample_1/depth_layer.0/norm1/Div, /down_sample_1/depth_layer.0/norm1/Mul, /down_sample_1/depth_layer.0/norm1/Add_1, /down_sample_1/depth_layer.1/norm2/ReduceMean_1, /down_sample_1/depth_layer.1/norm1/ReduceMean_1, /down_sample_3/depth_layer.0/attn/norm/Sub, /down_sample_4/rgb_stem/rgb_stem.0/Sub, /down_sample_3/depth_layer.0/attn/norm/Pow, /down_sample_4/rgb_stem/rgb_stem.0/Pow, /down_sample_3/depth_layer.0/attn/norm/ReduceMean_1, /down_sample_4/rgb_stem/rgb_stem.0/ReduceMean_1, /down_sample_3/depth_layer.0/attn/norm/Add, /down_sample_4/rgb_stem/rgb_stem.0/Add, /down_sample_4/rgb_stem/rgb_stem.0/Sqrt, /down_sample_3/depth_layer.0/attn/norm/Div, /down_sample_4/rgb_stem/rgb_stem.0/Div, /down_sample_3/depth_layer.0/attn/norm/Mul, /down_sample_4/rgb_stem/rgb_stem.0/Mul, /down_sample_3/depth_layer.0/attn/norm/Add_1, /down_sample_4/rgb_stem/rgb_stem.0/Add_1, /down_sample_4/rgb_layer/rgb_layer.0/norm/Sub, /down_sample_4/rgb_layer/rgb_layer.0/norm/Pow, /down_sample_4/rgb_layer/rgb_layer.0/norm/ReduceMean_1, /down_sample_4/rgb_layer/rgb_layer.0/norm/Add, /down_sample_4/rgb_layer/rgb_layer.0/norm/Sqrt, /down_sample_4/rgb_layer/rgb_layer.0/norm/Div, /down_sample_4/rgb_layer/rgb_layer.0/norm/Mul, /down_sample_4/rgb_layer/rgb_layer.0/norm/Add_1, /down_sample_3/depth_layer.0/norm2/Sub, /down_sample_3/depth_layer.0/norm2/Pow, /down_sample_3/depth_layer.0/norm2/ReduceMean_1, /down_sample_3/depth_layer.0/norm2/Add, /down_sample_3/depth_layer.0/norm2/Sqrt, /down_sample_3/depth_layer.0/norm2/Div, /down_sample_3/depth_layer.0/norm2/Mul, /down_sample_3/depth_layer.0/norm2/Add_1, /down_sample_4/rgb_layer/rgb_layer.1/norm/Sub, /down_sample_4/rgb_layer/rgb_layer.1/norm/Pow, /down_sample_4/rgb_layer/rgb_layer.1/norm/ReduceMean_1, /down_sample_4/rgb_layer/rgb_layer.1/norm/Add, /down_sample_4/rgb_layer/rgb_layer.1/norm/Sqrt, /down_sample_4/rgb_layer/rgb_layer.1/norm/Div, /down_sample_4/rgb_layer/rgb_layer.1/norm/Mul, /down_sample_4/rgb_layer/rgb_layer.1/norm/Add_1, /down_sample_3/depth_layer.1/norm1/Sub, /down_sample_3/depth_layer.1/norm1/Pow, /down_sample_3/depth_layer.1/norm1/ReduceMean_1, /down_sample_3/depth_layer.1/norm1/Add, /down_sample_3/depth_layer.1/norm1/Sqrt, /down_sample_3/depth_layer.1/norm1/Div, /down_sample_3/depth_layer.1/norm1/Mul, /down_sample_4/rgb_layer/rgb_layer.2/norm/Sub, /down_sample_3/depth_layer.1/norm1/Add_1, /down_sample_4/rgb_layer/rgb_layer.2/norm/Pow, /down_sample_4/rgb_layer/rgb_layer.2/norm/ReduceMean_1, /down_sample_4/rgb_layer/rgb_layer.2/norm/Add, /down_sample_4/rgb_layer/rgb_layer.2/norm/Sqrt, /down_sample_4/rgb_layer/rgb_layer.2/norm/Div, /down_sample_4/rgb_layer/rgb_layer.2/norm/Mul, /down_sample_4/rgb_layer/rgb_layer.2/norm/Add_1, /down_sample_3/depth_layer.1/attn/norm/Sub, /down_sample_3/depth_layer.1/attn/norm/Pow, /down_sample_3/depth_layer.1/attn/norm/ReduceMean_1, /down_sample_3/depth_layer.1/attn/norm/Add, /down_sample_3/depth_layer.1/attn/norm/Sqrt, /down_sample_3/depth_layer.1/attn/norm/Div, /down_sample_3/depth_layer.1/attn/norm/Mul, /down_sample_3/depth_layer.1/attn/norm/Add_1, /down_sample_3/depth_layer.1/norm2/Sqrt, /down_sample_4/depth_norm/Sqrt, /down_sample_4/depth_norm/Div, /down_sample_4/depth_norm/Mul, /down_sample_4/depth_norm/Add_1, /down_sample_3/rgb_stem/rgb_stem.0/Add, /down_sample_3/rgb_stem/rgb_stem.0/Sqrt, /down_sample_3/rgb_stem/rgb_stem.0/Div, /down_sample_3/rgb_stem/rgb_stem.0/Mul, /down_sample_2/depth_stem/norm/Sub, /down_sample_3/rgb_stem/rgb_stem.0/Add_1, /down_sample_2/depth_stem/norm/Pow, /down_sample_2/depth_stem/norm/ReduceMean_1, /down_sample_2/depth_stem/norm/Add, /down_sample_2/depth_stem/norm/Sqrt, /down_sample_2/depth_stem/norm/Div, /down_sample_3/rgb_layer/rgb_layer.0/norm/Sub, /down_sample_2/depth_stem/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.0/norm/Pow, /down_sample_2/depth_stem/norm/Add_1, /down_sample_3/rgb_layer/rgb_layer.0/norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.0/norm/Add, /down_sample_2/depth_layer.0/norm1/Sub, /down_sample_3/rgb_layer/rgb_layer.0/norm/Sqrt, /down_sample_2/depth_layer.0/norm1/Pow, /down_sample_3/rgb_layer/rgb_layer.0/norm/Div, /down_sample_2/depth_layer.0/norm1/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.0/norm/Mul, /down_sample_2/depth_layer.0/norm1/Add, /down_sample_3/rgb_layer/rgb_layer.0/norm/Add_1, /down_sample_2/depth_layer.0/norm1/Sqrt, /down_sample_2/depth_layer.0/norm1/Div, /down_sample_2/depth_layer.0/norm1/Mul, /down_sample_2/depth_layer.0/norm1/Add_1, /down_sample_2/depth_norm/Pow, /down_sample_2/depth_norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.6/norm/Sub, /down_sample_2/depth_norm/Add, /down_sample_3/rgb_layer/rgb_layer.6/norm/Pow, /down_sample_2/depth_norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.6/norm/ReduceMean_1, /down_sample_2/depth_norm/Div, /down_sample_3/rgb_layer/rgb_layer.6/norm/Add, /down_sample_2/depth_norm/Mul, /down_sample_3/rgb_layer/rgb_layer.6/norm/Sqrt, /down_sample_2/depth_norm/Add_1, /down_sample_3/rgb_layer/rgb_layer.6/norm/Div, /down_sample_3/rgb_layer/rgb_layer.6/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.6/norm/Add_1, /down_sample_3/depth_layer.0/attn/norm/Sqrt, /down_sample_3/depth_layer.1/norm2/Sub, /down_sample_3/depth_layer.1/norm2/Pow, /down_sample_3/depth_layer.1/norm2/ReduceMean_1, /down_sample_3/depth_layer.1/norm2/Add, /down_sample_3/depth_layer.1/norm2/Div, /down_sample_3/depth_layer.1/norm2/Mul, /down_sample_3/depth_layer.1/norm2/Add_1, /down_sample_3/depth_norm/Sub, /down_sample_3/depth_norm/Pow, /down_sample_3/depth_norm/ReduceMean_1, /down_sample_3/depth_norm/Add, /down_sample_3/depth_norm/Sqrt, /down_sample_3/depth_norm/Div, /down_sample_3/depth_norm/Mul, /down_sample_3/depth_norm/Add_1, /down_sample_1/depth_layer.1/attn/norm/ReduceMean_1, /down_sample_4/depth_stem/norm/Sqrt, /down_sample_4/depth_stem/norm/Div, /down_sample_4/depth_stem/norm/Mul, /down_sample_4/depth_stem/norm/Add_1, /down_sample_4/depth_layer.0/norm1/Sub, /down_sample_4/depth_layer.0/norm1/Pow, /down_sample_4/depth_layer.0/norm1/ReduceMean_1, /down_sample_4/depth_layer.0/norm1/Add, /down_sample_4/depth_layer.0/norm1/Sqrt, /down_sample_4/depth_layer.0/norm1/Div, /down_sample_4/depth_layer.0/norm1/Mul, /down_sample_4/depth_layer.0/norm1/Add_1, /down_sample_4/depth_layer.0/norm2/Sub, /down_sample_4/depth_layer.0/norm2/Pow, /down_sample_4/depth_layer.0/norm2/ReduceMean_1, /down_sample_4/depth_layer.0/norm2/Add, /down_sample_4/depth_layer.0/norm2/Sqrt, /down_sample_4/depth_layer.0/norm2/Div, /down_sample_4/depth_layer.0/norm2/Mul, /down_sample_4/depth_layer.0/norm2/Add_1, /down_sample_3/depth_stem/norm/Sub, /down_sample_3/depth_stem/norm/Pow, /down_sample_3/depth_stem/norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.8/norm/Sub, /down_sample_3/depth_stem/norm/Add, /down_sample_3/rgb_layer/rgb_layer.8/norm/Pow, /down_sample_3/depth_stem/norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.8/norm/ReduceMean_1, /down_sample_3/depth_stem/norm/Div, /down_sample_3/rgb_layer/rgb_layer.8/norm/Add, /down_sample_3/depth_stem/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.8/norm/Sqrt, /down_sample_3/depth_stem/norm/Add_1, /down_sample_3/rgb_layer/rgb_layer.8/norm/Div, /down_sample_3/rgb_layer/rgb_layer.8/norm/Mul, /down_sample_3/depth_layer.0/norm1/Sub, /down_sample_3/rgb_layer/rgb_layer.8/norm/Add_1, /down_sample_3/depth_layer.0/norm1/Pow, /down_sample_3/depth_layer.0/norm1/ReduceMean_1, /down_sample_3/depth_layer.0/norm1/Add, /down_sample_3/depth_layer.0/norm1/Sqrt, /down_sample_3/depth_layer.0/norm1/Div, /down_sample_3/depth_layer.0/norm1/Mul, /down_sample_3/depth_layer.0/norm1/Add_1, /down_sample_3/rgb_layer/rgb_layer.7/norm/Sub, /down_sample_3/rgb_layer/rgb_layer.7/norm/Pow, /down_sample_3/rgb_layer/rgb_layer.7/norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.7/norm/Add, /down_sample_3/rgb_layer/rgb_layer.7/norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.7/norm/Div, /down_sample_3/rgb_layer/rgb_layer.7/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.7/norm/Add_1, /down_sample_3/rgb_stem/rgb_stem.0/ReduceMean_1, /down_sample_2/depth_layer.0/attn/norm/Sub, /down_sample_2/depth_layer.0/attn/norm/Pow, /down_sample_2/depth_layer.0/attn/norm/ReduceMean_1, /down_sample_2/depth_layer.0/attn/norm/Add, /down_sample_2/depth_layer.0/attn/norm/Sqrt, /down_sample_2/depth_layer.0/attn/norm/Div, /down_sample_3/rgb_layer/rgb_layer.1/norm/Sub, /down_sample_2/depth_layer.0/attn/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.1/norm/Pow, /down_sample_2/depth_layer.0/attn/norm/Add_1, /down_sample_3/rgb_layer/rgb_layer.1/norm/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.1/norm/Add, /down_sample_3/rgb_layer/rgb_layer.1/norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.1/norm/Div, /down_sample_3/rgb_layer/rgb_layer.1/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.1/norm/Add_1, /down_sample_2/depth_layer.0/norm2/Sub, /down_sample_2/depth_layer.0/norm2/Pow, /down_sample_2/depth_layer.0/norm2/Add, /down_sample_3/rgb_layer/rgb_layer.2/norm/Sub, /down_sample_2/depth_layer.0/norm2/Sqrt, /down_sample_3/rgb_layer/rgb_layer.2/norm/Pow, /down_sample_2/depth_layer.0/norm2/Div, /down_sample_3/rgb_layer/rgb_layer.2/norm/ReduceMean_1, /down_sample_2/depth_layer.0/norm2/Mul, /down_sample_3/rgb_layer/rgb_layer.2/norm/Add, /down_sample_2/depth_layer.0/norm2/Add_1, /down_sample_3/rgb_layer/rgb_layer.2/norm/Sqrt, /down_sample_3/rgb_layer/rgb_layer.2/norm/Div, /down_sample_3/rgb_layer/rgb_layer.2/norm/Mul, /down_sample_3/rgb_layer/rgb_layer.2/norm/Add_1, /down_sample_3/rgb_layer/rgb_layer.3/norm/Sub, /down_sample_2/depth_layer.1/norm1/Sub, /down_sample_3/rgb_layer/rgb_layer.3/norm/Pow, /down_sample_2/depth_layer.1/norm1/Pow, /down_sample_3/rgb_layer/rgb_layer.3/norm/ReduceMean_1, /down_sample_2/depth_layer.1/norm1/ReduceMean_1, /down_sample_3/rgb_layer/rgb_layer.3/norm/Add, /down_sample_2/depth_layer.1/norm1/Add, /down_sample_3/rgb_layer/rgb_layer.3/norm/Sqrt, /down_sample_2/depth_layer.1/norm1/Sqrt, /down_sample_3/rgb_layer/rgb_layer.3/norm/Div, /down_sample_2/depth_layer.1/norm1/Div, /down_sample_3/rgb_layer/rgb_layer.3/norm/Mul, /down_sample_2/depth_layer.1/norm1/Mul, /down_sample_3/rgb_layer/rgb_layer.3/norm/Add_1, /down_sample_2/depth_layer.1/norm1/Add_1, /down_sample_2/rgb_layer/rgb_layer.2/norm/Sub, /down_sample_2/rgb_layer/rgb_layer.2/norm/Pow, /down_sample_2/rgb_layer/rgb_layer.2/norm/Add, /down_sample_2/rgb_layer/rgb_layer.2/norm/Sqrt, /down_sample_2/rgb_layer/rgb_layer.2/norm/Div, /down_sample_2/rgb_layer/rgb_layer.2/norm/Mul, /down_sample_2/rgb_layer/rgb_layer.2/norm/Add_1, /down_sample_1/depth_norm/Sub, /down_sample_1/depth_norm/Pow, /down_sample_1/depth_norm/Add, /down_sample_1/depth_norm/Sqrt, /down_sample_1/depth_norm/Div, /down_sample_1/depth_norm/Mul, /down_sample_1/depth_norm/Add_1, /down_sample_3/rgb_stem/rgb_stem.0/Sub, /down_sample_3/rgb_stem/rgb_stem.0/Pow, /down_sample_1/depth_stem/norm/Sqrt\n",
      "[06/20/2024-18:38:16] [TRT] [W] Running layernorm after self-attention in FP16 may cause overflow. Exporting the model to the latest available ONNX opset (later than opset 17) to use the INormalizationLayer, or forcing layernorm layers to run in FP32 precision can help with preserving accuracy.\n",
      "[06/20/2024-18:45:18] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[06/20/2024-18:45:18] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[06/20/2024-18:45:18] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[06/20/2024-18:45:18] [TRT] [W] - 233 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[06/20/2024-18:45:18] [TRT] [W] - 26 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "Serialized Engine Saved at:  ./models/asymformer-v6.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt.tensorrt.ICudaEngine at 0x7f3be8ce63f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ONNX_SIM_MODEL_PATH = model_path_output\n",
    "\n",
    "TENSORRT_ENGINE_PATH_PY = \"./models/\"+name+'.engine'\n",
    "\n",
    "build_engine(ONNX_SIM_MODEL_PATH, TENSORRT_ENGINE_PATH_PY, overflow_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5.3.1\n"
     ]
    }
   ],
   "source": [
    "print(trt.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
